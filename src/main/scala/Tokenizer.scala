package nltk

class Tokenizer(sents: Sentenceizer) {
  val tokens = List("I","compute","therefore","I","am")
}
